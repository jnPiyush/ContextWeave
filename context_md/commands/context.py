"""
Context Command - Generate and manage task context

Usage:
    context-md context generate <issue>
    context-md context show <issue>
    context-md context refresh <issue>

Implements the 4-Layer AI Context Architecture:
1. System Context - Role instructions (governs AI behavior)
2. User Prompt - Issue details (what user asks)
3. Memory - Lessons learned, session history (persistent info)
4. Retrieval Context - Skills/docs (knowledge grounding)
"""

from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

import click

from context_md.config import Config
from context_md.memory import Memory
from context_md.state import State

# Context file template with 4-Layer architecture + Enhanced Prompt
CONTEXT_TEMPLATE = '''# Context - Issue #{issue}

## Metadata
| Field | Value |
|-------|-------|
| Generated | {timestamp} |
| Issue | #{issue} |
| Type | {issue_type} |
| Role | {role} |
| Branch | {branch} |
| Skills | {skills} |
| Prompt Quality | {prompt_quality} |

---

## Layer 1: System Context (Governs Behavior)

{role_instructions}

---

## Layer 2: User Prompt (Enhanced with Prompt Engineering)

{enhanced_prompt}

### Original Request

{description}

---

## Layer 3: Memory (Persistent Knowledge)

{memory_context}

---

## Layer 4: Retrieval Context (Knowledge Grounding)

{skill_content}

---

## References

{references}

---

## Execution Checklist

- [ ] Review this context completely
- [ ] Understand the enhanced prompt structure
- [ ] Check Layer 3 Memory for lessons learned
- [ ] Follow the suggested approach
- [ ] Avoid listed pitfalls
- [ ] Execute work meeting success criteria
- [ ] Complete quality checklist
- [ ] Prepare handoff for next role
- [ ] Record outcome to Memory layer
- [ ] Update status

---

*Generated by Context.md v{version} | 4-Layer AI Context Architecture + Prompt Engineering*
'''


@click.group("context")
def context_cmd() -> None:
    """Generate and manage task context files.
    
    Context files contain everything a SubAgent needs to execute a task:
    - Role instructions
    - Issue details
    - Relevant skills
    - Dependencies
    - References
    """


@context_cmd.command("generate")
@click.argument("issue", type=int)
@click.option("--output", "-o", type=click.Path(), help="Output path (default: .agent-context/context-{issue}.md)")
@click.option("--role", type=click.Choice(["pm", "architect", "engineer", "reviewer", "ux"]),
              help="Override role detection")
@click.option("--dry-run", is_flag=True, help="Generate but don't write")
@click.pass_context
def generate_cmd(ctx: click.Context, issue: int, output: Optional[str],
                 role: Optional[str], dry_run: bool) -> None:
    """Generate context file for an issue.
    
    This assembles:
    - Role instructions from .github/agents/
    - Issue details from Git branch/notes or GitHub
    - Relevant skills based on labels
    - Memory context (lessons learned, session history)
    - Dependencies and references
    
    Example:
        context-md context generate 456
        context-md context generate 456 --role engineer
    """
    from context_md import __version__

    repo_root = ctx.obj.get("repo_root")
    if not repo_root:
        raise click.ClickException("Not in a Git repository with Context.md initialized.")

    state = ctx.obj.get("state", State(repo_root))
    config = ctx.obj.get("config", Config(repo_root))
    memory = Memory(repo_root)  # Initialize memory layer
    verbose = ctx.obj.get("verbose", False)

    click.echo(f"Generating context for issue #{issue}...")

    # Get worktree info if exists
    worktree = state.get_worktree(issue)
    branch = worktree.branch if worktree else f"issue-{issue}-*"

    # Find the actual branch
    if not worktree:
        branches = state.get_issue_branches()
        matching = [b for b in branches if b.startswith(f"issue-{issue}-")]
        if matching:
            branch = matching[0]
        else:
            branch = f"issue-{issue}"

    # Get metadata from Git notes
    metadata = state.get_branch_note(branch) if branch else {}

    # Determine role
    if role:
        detected_role = role
    elif worktree:
        detected_role = worktree.role
    elif metadata:
        detected_role = metadata.get("role", "engineer")
    else:
        detected_role = "engineer"

    # Load role instructions (Layer 1: System Context)
    role_instructions = load_role_instructions(repo_root, detected_role)

    # Determine issue type and labels from branch name or metadata
    issue_type = metadata.get("type", "story")
    labels = metadata.get("labels", [])

    # If no labels, infer from branch name
    if not labels and branch:
        if "bug" in branch.lower() or "fix" in branch.lower():
            issue_type = "bug"
            labels = ["type:bug"]
        elif "feature" in branch.lower():
            issue_type = "feature"
            labels = ["type:feature"]
        else:
            labels = ["type:story"]

    # Get relevant skills (Layer 4: Retrieval Context)
    skill_numbers = config.get_skills_for_labels(labels)
    skill_content = load_skills(repo_root, skill_numbers, verbose)

    # Get memory context (Layer 3: Memory)
    # Extract categories from labels for memory lookup
    categories = [label.replace("type:", "") for label in labels if ":" in label]
    if not categories:
        categories = [issue_type]
    memory_context = memory.get_memory_context(issue, issue_type, detected_role, categories)

    # Enhanced User Prompt (Layer 2 with Prompt Engineering)
    from context_md.prompt import PromptEngineer
    prompt_engineer = PromptEngineer()
    
    # Build context for prompt enhancement
    prompt_context = {
        "dependencies": metadata.get("dependencies", []),
        "previous_session": memory.get_latest_session(issue),
    }
    
    # Check for existing PRD/Spec
    prd_path = repo_root / "docs" / "prd" / f"PRD-{issue}.md"
    spec_path = repo_root / "docs" / "specs" / f"SPEC-{issue}.md"
    if prd_path.exists():
        prompt_context["prd_path"] = f"docs/prd/PRD-{issue}.md"
    if spec_path.exists():
        prompt_context["spec_path"] = f"docs/specs/SPEC-{issue}.md"
    
    # Get raw description
    raw_description = metadata.get("description", f"Complete issue #{issue}")
    
    # Enhance the prompt
    enhanced = prompt_engineer.enhance_prompt(
        raw_prompt=raw_description,
        role=detected_role,
        issue_number=issue,
        issue_type=issue_type,
        labels=labels,
        context=prompt_context
    )
    
    # Validate prompt completeness
    validation = prompt_engineer.validate_prompt_completeness(enhanced)
    prompt_quality = f"{validation['completeness_score']:.0%}"
    if not validation["is_valid"]:
        prompt_quality += " ⚠️"

    # Build context content
    context_data = {
        "issue": issue,
        "timestamp": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "issue_type": issue_type,
        "role": detected_role,
        "branch": branch,
        "skills": ", ".join(skill_numbers) if skill_numbers else "Default (#02, #04, #11)",
        "prompt_quality": prompt_quality,
        "role_instructions": role_instructions,
        "enhanced_prompt": enhanced.to_markdown(),
        "description": raw_description,
        "memory_context": memory_context,
        "skill_content": skill_content,
        "references": format_references(metadata.get("references", [])),
        "version": __version__
    }

    content = CONTEXT_TEMPLATE.format(**context_data)

    if dry_run:
        click.echo("")
        click.echo(content)
        return

    # Determine output path
    if output:
        output_path = Path(output)
    else:
        output_path = repo_root / ".agent-context" / f"context-{issue}.md"

    # Write context file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content, encoding="utf-8")

    click.echo("")
    click.secho(f"[OK] Context generated: {output_path}", fg="green")
    click.echo("")
    click.echo(f"   Role: {detected_role}")
    click.echo(f"   Skills: {', '.join(skill_numbers)}")
    click.echo(f"   Prompt Quality: {prompt_quality}")
    click.echo(f"   Memory: {len(memory_context)} chars")
    click.echo(f"   Size: {len(content):,} characters")

    # Show validation warnings
    if validation["warnings"]:
        click.echo("")
        click.secho("   Prompt Warnings:", fg="yellow")
        for warning in validation["warnings"][:3]:
            click.echo(f"   ⚠️  {warning}")

    # Estimate tokens (rough: ~4 chars per token)
    estimated_tokens = len(content) // 4
    click.echo(f"   Est. tokens: ~{estimated_tokens:,}")


@context_cmd.command("show")
@click.argument("issue", type=int)
@click.option("--metadata", is_flag=True, help="Show only metadata")
@click.pass_context
def show_cmd(ctx: click.Context, issue: int, metadata: bool) -> None:
    """Show context file for an issue."""
    repo_root = ctx.obj.get("repo_root")
    if not repo_root:
        raise click.ClickException("Not in a Git repository with Context.md initialized.")

    context_path = repo_root / ".agent-context" / f"context-{issue}.md"

    if not context_path.exists():
        raise click.ClickException(
            f"Context file not found: {context_path}\n"
            f"Generate it with: context-md context generate {issue}"
        )

    content = context_path.read_text(encoding="utf-8")

    if metadata:
        # Extract just the metadata section
        lines = content.split("\n")
        in_metadata = False
        for line in lines:
            if line.startswith("## Metadata"):
                in_metadata = True
            elif in_metadata and line.startswith("## "):
                break
            elif in_metadata:
                click.echo(line)
    else:
        click.echo(content)


@context_cmd.command("refresh")
@click.argument("issue", type=int)
@click.pass_context
def refresh_cmd(ctx: click.Context, issue: int) -> None:
    """Regenerate context file for an issue."""
    ctx.invoke(generate_cmd, issue=issue)


def load_role_instructions(repo_root: Path, role: str) -> str:
    """Load role instructions from .github/agents/"""
    role_files = {
        "pm": "product-manager.agent.md",
        "architect": "architect.agent.md",
        "engineer": "engineer.agent.md",
        "reviewer": "reviewer.agent.md",
        "ux": "ux-designer.agent.md"
    }

    filename = role_files.get(role, "engineer.agent.md")
    agent_file = repo_root / ".github" / "agents" / filename

    if agent_file.exists():
        return agent_file.read_text(encoding="utf-8")

    # Fallback to basic instructions
    return f"""### {role.title()} Role

You are acting as a {role.title()} agent.

Follow the standard AgentX workflow:
1. Review the issue and context
2. Create an execution plan
3. Execute the work
4. Validate against Definition of Done
5. Update status and handoff

Refer to AGENTS.md and Skills.md for detailed guidelines.
"""


def load_skills(repo_root: Path, skill_numbers: List[str], verbose: bool) -> str:
    """Load skill content from .github/skills/"""
    skills_base = repo_root / ".github" / "skills"

    # Skill number to path mapping
    skill_paths = {
        "#01": "architecture/core-principles/SKILL.md",
        "#02": "development/testing/SKILL.md",
        "#03": "development/error-handling/SKILL.md",
        "#04": "architecture/security/SKILL.md",
        "#05": "architecture/performance/SKILL.md",
        "#06": "architecture/database/SKILL.md",
        "#07": "architecture/scalability/SKILL.md",
        "#08": "architecture/code-organization/SKILL.md",
        "#09": "architecture/api-design/SKILL.md",
        "#10": "development/configuration/SKILL.md",
        "#11": "development/documentation/SKILL.md",
        "#12": "development/version-control/SKILL.md",
        "#13": "development/type-safety/SKILL.md",
        "#14": "development/dependency-management/SKILL.md",
        "#15": "development/logging-monitoring/SKILL.md",
        "#16": "operations/remote-git-operations/SKILL.md",
        "#17": "ai-systems/ai-agent-development/SKILL.md",
        "#18": "development/code-review-and-audit/SKILL.md",
        "#19": "development/csharp/SKILL.md",
        "#20": "development/python/SKILL.md",
        "#21": "development/frontend-ui/SKILL.md",
        "#22": "development/react/SKILL.md",
        "#23": "development/blazor/SKILL.md",
        "#24": "development/postgresql/SKILL.md",
        "#25": "development/sql-server/SKILL.md"
    }

    content_parts = []

    for skill_num in skill_numbers:
        skill_path = skill_paths.get(skill_num)
        if not skill_path:
            if verbose:
                click.echo(f"  Warning: Unknown skill {skill_num}")
            continue

        full_path = skills_base / skill_path
        if full_path.exists():
            skill_content = full_path.read_text(encoding="utf-8")
            skill_name = skill_path.split("/")[-2].replace("-", " ").title()
            content_parts.append(f"### Skill {skill_num}: {skill_name}\n\n{skill_content}")
            if verbose:
                click.echo(f"  Loaded skill: {skill_num}")
        else:
            if verbose:
                click.echo(f"  Warning: Skill file not found: {full_path}")

    return "\n\n---\n\n".join(content_parts) if content_parts else "No skills loaded."


def format_dependencies(dependencies: List[Dict]) -> str:
    """Format dependencies list."""
    if not dependencies:
        return "No dependencies specified."

    parts = []
    for dep in dependencies:
        if isinstance(dep, dict):
            parts.append(f"""### #{dep.get('issue', 'N/A')} - {dep.get('title', 'Unknown')}
| Field | Value |
|-------|-------|
| **Provides** | {dep.get('provides', 'Not specified')} |
| **Integration** | {dep.get('integration', 'Not specified')} |
| **Verification** | {dep.get('verification', 'Not specified')} |
""")
        else:
            parts.append(f"- {dep}")

    return "\n".join(parts)


def format_acceptance_criteria(criteria: List[str]) -> str:
    """Format acceptance criteria."""
    if not criteria:
        return "- [ ] No acceptance criteria specified"

    return "\n".join(f"- [ ] {c}" for c in criteria)


def format_references(references: List[Dict]) -> str:
    """Format references list."""
    if not references:
        return "No references specified."

    parts = ["### Code Files"]
    for ref in references:
        if isinstance(ref, dict):
            parts.append(f"- `{ref.get('path', 'unknown')}` - {ref.get('purpose', 'No description')}")
        else:
            parts.append(f"- `{ref}`")

    return "\n".join(parts)
