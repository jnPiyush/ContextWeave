"""
Context Command - Generate and manage task context

Usage:
    context-weave context generate <issue>
    context-weave context show <issue>
    context-weave context refresh <issue>

Implements the 4-Layer AI Context Architecture:
1. System Context - Role instructions (governs AI behavior)
2. User Prompt - Issue details (what user asks)
3. Memory - Lessons learned, session history (persistent info)
4. Retrieval Context - Skills/docs (knowledge grounding)
"""

from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

import click

from context_weave.config import Config
from context_weave.memory import Memory
from context_weave.state import State

# Context file template with 4-Layer architecture + Enhanced Prompt
CONTEXT_TEMPLATE = '''# Context - Issue #{issue}

## Metadata
| Field | Value |
|-------|-------|
| Generated | {timestamp} |
| Issue | #{issue} |
| Type | {issue_type} |
| Role | {role} |
| Branch | {branch} |
| Skills | {skills} |
| Prompt Quality | {prompt_quality} |

---

## Layer 1: System Context (Governs Behavior)

{role_instructions}

---

## Layer 2: User Prompt (Enhanced with Prompt Engineering)

{enhanced_prompt}

### Original Request

{description}

---

## Layer 3: Memory (Persistent Knowledge)

{memory_context}

---

## Layer 4: Retrieval Context (Knowledge Grounding)

{skill_content}

---

## References

{references}

---

## Execution Checklist

- [ ] Review this context completely
- [ ] Understand the enhanced prompt structure
- [ ] Check Layer 3 Memory for lessons learned
- [ ] Follow the suggested approach
- [ ] Avoid listed pitfalls
- [ ] Execute work meeting success criteria
- [ ] Complete quality checklist
- [ ] Prepare handoff for next role
- [ ] Record outcome to Memory layer
- [ ] Update status

---

*Generated by ContextWeave v{version} | 4-Layer AI Context Architecture + Prompt Engineering*
'''


@click.group("context")
def context_cmd() -> None:
    """Generate and manage task context files.

    Context files contain everything a SubAgent needs to execute a task:
    - Role instructions
    - Issue details
    - Relevant skills
    - Dependencies
    - References
    """


def generate_context_file(
    issue: int,
    repo_root: Path,
    state: State,
    config: Config,
    role: str = None,
    output: Path = None,
    verbose: bool = False
) -> tuple[Path, dict]:
    """Generate context file for an issue (extracted for reuse).

    Args:
        issue: Issue number
        repo_root: Repository root path
        state: State object
        config: Config object
        role: Role to use (auto-detected if None)
        output: Output file path (default: .context-weave/context-{issue}.md)
        verbose: Verbose output for skill loading

    Returns:
        Tuple of (output_path, stats_dict) where stats includes:
        - role, skills, prompt_quality, memory_size, content_size, estimated_tokens, warnings
    """
    from context_weave import __version__
    from context_weave.prompt import PromptEngineer

    memory = Memory(repo_root)
    worktree = state.get_worktree(issue)
    branch = worktree.branch if worktree else f"issue-{issue}"

    # Get metadata from Git notes or state
    metadata = state.get_branch_note(branch) or {}
    if not metadata:
        metadata = state.local_issues.get(str(issue), {})

    # Determine role
    if role:
        detected_role = role
    elif worktree:
        detected_role = worktree.role
    elif metadata:
        detected_role = metadata.get("role", "engineer")
    else:
        detected_role = "engineer"

    # Load role instructions (Layer 1: System Context)
    role_instructions = load_role_instructions(repo_root, detected_role)

    # Determine issue type and labels
    issue_type = metadata.get("type", "story")
    labels = metadata.get("labels", [])

    # If no labels, infer from branch name
    if not labels and branch:
        if "bug" in branch.lower() or "fix" in branch.lower():
            issue_type = "bug"
            labels = ["type:bug"]
        elif "feature" in branch.lower():
            issue_type = "feature"
            labels = ["type:feature"]
        else:
            labels = ["type:story"]

    # Get relevant skills (Layer 4: Retrieval Context)
    skill_names = config.get_skills_for_labels(labels)
    skill_content = load_skills(repo_root, skill_names, verbose, config.max_skill_tokens)

    # Get memory context (Layer 3: Memory)
    categories = [label.replace("type:", "") for label in labels if ":" in label]
    if not categories:
        categories = [issue_type]
    memory_context = memory.get_memory_context(issue, issue_type, detected_role, categories)

    # Enhanced User Prompt (Layer 2)
    prompt_engineer = PromptEngineer()
    prompt_context = {
        "dependencies": metadata.get("dependencies", []),
        "previous_session": memory.get_latest_session(issue),
    }

    # Check for existing PRD/Spec
    prd_path = repo_root / "docs" / "prd" / f"PRD-{issue}.md"
    spec_path = repo_root / "docs" / "specs" / f"SPEC-{issue}.md"
    if prd_path.exists():
        prompt_context["prd_path"] = f"docs/prd/PRD-{issue}.md"
    if spec_path.exists():
        prompt_context["spec_path"] = f"docs/specs/SPEC-{issue}.md"

    raw_description = metadata.get("description", f"Complete issue #{issue}")

    # Enhance the prompt
    enhanced = prompt_engineer.enhance_prompt(
        raw_prompt=raw_description,
        role=detected_role,
        issue_number=issue,
        issue_type=issue_type,
        labels=labels,
        context=prompt_context
    )

    # Validate prompt completeness
    validation = prompt_engineer.validate_prompt_completeness(enhanced)
    prompt_quality = f"{validation['completeness_score']:.0%}"
    if not validation["is_valid"]:
        prompt_quality += " [!]"

    # Build context content
    context_data = {
        "issue": issue,
        "timestamp": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "issue_type": issue_type,
        "role": detected_role,
        "branch": branch,
        "skills": ", ".join(skill_names) if skill_names else "Default (testing, security, documentation)",
        "prompt_quality": prompt_quality,
        "role_instructions": role_instructions,
        "enhanced_prompt": enhanced.to_markdown(),
        "description": raw_description,
        "memory_context": memory_context,
        "skill_content": skill_content,
        "references": format_references(metadata.get("references", [])),
        "version": __version__
    }

    content = CONTEXT_TEMPLATE.format(**context_data)

    # Determine output path
    if output:
        output_path = output
    else:
        output_path = repo_root / ".context-weave" / f"context-{issue}.md"

    # Write context file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content, encoding="utf-8")

    # Return stats
    stats = {
        "role": detected_role,
        "skills": skill_names,
        "prompt_quality": prompt_quality,
        "memory_size": len(memory_context),
        "content_size": len(content),
        "estimated_tokens": len(content) // 4,
        "warnings": validation["warnings"]
    }

    return output_path, stats


@context_cmd.command("generate")
@click.argument("issue", type=int)
@click.option("--output", "-o", type=click.Path(), help="Output path (default: .context-weave/context-{issue}.md)")
@click.option("--role", type=click.Choice(["pm", "architect", "engineer", "reviewer", "ux"]),
              help="Override role detection")
@click.option("--dry-run", is_flag=True, help="Generate but don't write")
@click.pass_context
def generate_cmd(ctx: click.Context, issue: int, output: Optional[str],
                 role: Optional[str], dry_run: bool) -> None:
    """Generate context file for an issue.

    This assembles:
    - Role instructions from .github/agents/
    - Issue details from Git branch/notes or GitHub
    - Relevant skills based on labels
    - Memory context (lessons learned, session history)
    - Dependencies and references

    Example:
        context-weave context generate 456
        context-weave context generate 456 --role engineer
    """
    repo_root = ctx.obj.get("repo_root")
    if not repo_root:
        raise click.ClickException("Not in a Git repository with ContextWeave initialized.")

    state = ctx.obj.get("state", State(repo_root))
    config = ctx.obj.get("config", Config(repo_root))
    verbose = ctx.obj.get("verbose", False)

    click.echo(f"Generating context for issue #{issue}...")

    # Handle dry-run mode (preview content without writing file)
    if dry_run:
        # Temporarily generate to get content, but don't write
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as temp:
            temp_path = Path(temp.name)

        try:
            output_path, stats = generate_context_file(
                issue, repo_root, state, config, role, temp_path, verbose
            )
            content = output_path.read_text(encoding="utf-8")
            click.echo("")
            click.echo(content)
        finally:
            temp_path.unlink(missing_ok=True)
        return

    # Normal mode: generate and write
    output_path, stats = generate_context_file(
        issue, repo_root, state, config, role,
        Path(output) if output else None, verbose
    )

    click.echo("")
    click.secho(f"[OK] Context generated: {output_path}", fg="green")
    click.echo("")
    click.echo(f"   Role: {stats['role']}")
    click.echo(f"   Skills: {', '.join(stats['skills'])}")
    click.echo(f"   Prompt Quality: {stats['prompt_quality']}")
    click.echo(f"   Memory: {stats['memory_size']} chars")
    click.echo(f"   Size: {stats['content_size']:,} characters")

    # Show validation warnings
    if stats['warnings']:
        click.echo("")
        click.secho("   Prompt Warnings:", fg="yellow")
        for warning in stats['warnings'][:3]:
            click.echo(f"   [!] {warning}")

    click.echo(f"   Est. tokens: ~{stats['estimated_tokens']:,}")


@context_cmd.command("show")
@click.argument("issue", type=int)
@click.option("--metadata", is_flag=True, help="Show only metadata")
@click.pass_context
def show_cmd(ctx: click.Context, issue: int, metadata: bool) -> None:
    """Show context file for an issue."""
    repo_root = ctx.obj.get("repo_root")
    if not repo_root:
        raise click.ClickException("Not in a Git repository with ContextWeave initialized.")

    context_path = repo_root / ".context-weave" / f"context-{issue}.md"

    if not context_path.exists():
        raise click.ClickException(
            f"Context file not found: {context_path}\n"
            f"Generate it with: context-weave context generate {issue}"
        )

    content = context_path.read_text(encoding="utf-8")

    if metadata:
        # Extract just the metadata section
        lines = content.split("\n")
        in_metadata = False
        for line in lines:
            if line.startswith("## Metadata"):
                in_metadata = True
            elif in_metadata and line.startswith("## "):
                break
            elif in_metadata:
                click.echo(line)
    else:
        click.echo(content)


@context_cmd.command("refresh")
@click.argument("issue", type=int)
@click.pass_context
def refresh_cmd(ctx: click.Context, issue: int) -> None:
    """Regenerate context file for an issue."""
    ctx.invoke(generate_cmd, issue=issue)


def load_role_instructions(repo_root: Path, role: str) -> str:
    """Load role instructions from .github/agents/"""
    role_files = {
        "pm": "product-manager.agent.md",
        "architect": "architect.agent.md",
        "engineer": "engineer.agent.md",
        "reviewer": "reviewer.agent.md",
        "ux": "ux-designer.agent.md"
    }

    filename = role_files.get(role, "engineer.agent.md")
    agent_file = repo_root / ".github" / "agents" / filename

    if agent_file.exists():
        return agent_file.read_text(encoding="utf-8")

    # Fallback to basic instructions
    return f"""### {role.title()} Role

You are acting as a {role.title()} agent.

Follow the standard AgentX workflow:
1. Review the issue and context
2. Create an execution plan
3. Execute the work
4. Validate against Definition of Done
5. Update status and handoff

Refer to AGENTS.md and Skills.md for detailed guidelines.
"""


def _discover_skills(repo_root: Path) -> Dict[str, str]:
    """Scan .github/skills/ and build name-to-path map.

    The directory structure is:
        .github/skills/{category}/{skill-name}/SKILL.md

    Returns dict mapping skill name (e.g. "testing") to the relative
    path from the skills base directory (e.g. "development/testing/SKILL.md").
    """
    skills_base = repo_root / ".github" / "skills"
    skill_map: Dict[str, str] = {}
    if not skills_base.exists():
        return skill_map
    for skill_file in skills_base.rglob("SKILL.md"):
        rel = skill_file.relative_to(skills_base)
        skill_name = skill_file.parent.name
        skill_map[skill_name] = str(rel)
    return skill_map


def load_skills(
    repo_root: Path,
    skill_names: List[str],
    verbose: bool,
    max_tokens: int = 8000,
) -> str:
    """Load skill content from .github/skills/ by name, respecting token budget.

    Args:
        repo_root: Repository root path
        skill_names: List of skill names (e.g. ["testing", "security"])
        verbose: Whether to print loading details
        max_tokens: Approximate token budget (~4 chars per token)
    """
    skills_base = repo_root / ".github" / "skills"
    skill_map = _discover_skills(repo_root)

    content_parts: List[str] = []
    chars_used = 0
    chars_limit = max_tokens * 4  # rough: ~4 chars per token
    truncated = False

    for skill_name in skill_names:
        skill_path = skill_map.get(skill_name)
        if not skill_path:
            if verbose:
                click.echo(f"  Warning: Unknown skill '{skill_name}'")
            continue

        full_path = skills_base / skill_path
        if full_path.exists():
            skill_content = full_path.read_text(encoding="utf-8")

            # Check budget
            if chars_used + len(skill_content) > chars_limit and content_parts:
                truncated = True
                if verbose:
                    click.echo(f"  Skill budget reached, skipping: {skill_name}")
                continue

            chars_used += len(skill_content)
            display_name = skill_name.replace("-", " ").title()
            content_parts.append(f"### Skill: {display_name}\n\n{skill_content}")
            if verbose:
                click.echo(f"  Loaded skill: {skill_name}")
        elif verbose:
            click.echo(f"  Warning: Skill file not found: {full_path}")

    if truncated:
        content_parts.append(
            "_Note: Some skills were omitted to stay within the token budget. "
            "Adjust `max_skill_tokens` in config to change the limit._"
        )

    return "\n\n---\n\n".join(content_parts) if content_parts else "No skills loaded."


def format_dependencies(dependencies: List[Dict]) -> str:
    """Format dependencies list."""
    if not dependencies:
        return "No dependencies specified."

    parts = []
    for dep in dependencies:
        if isinstance(dep, dict):
            parts.append(f"""### #{dep.get('issue', 'N/A')} - {dep.get('title', 'Unknown')}
| Field | Value |
|-------|-------|
| **Provides** | {dep.get('provides', 'Not specified')} |
| **Integration** | {dep.get('integration', 'Not specified')} |
| **Verification** | {dep.get('verification', 'Not specified')} |
""")
        else:
            parts.append(f"- {dep}")

    return "\n".join(parts)


def format_acceptance_criteria(criteria: List[str]) -> str:
    """Format acceptance criteria."""
    if not criteria:
        return "- [ ] No acceptance criteria specified"

    return "\n".join(f"- [ ] {c}" for c in criteria)


def format_references(references: List[Dict]) -> str:
    """Format references list."""
    if not references:
        return "No references specified."

    parts = ["### Code Files"]
    for ref in references:
        if isinstance(ref, dict):
            parts.append(f"- `{ref.get('path', 'unknown')}` - {ref.get('purpose', 'No description')}")
        else:
            parts.append(f"- `{ref}`")

    return "\n".join(parts)
